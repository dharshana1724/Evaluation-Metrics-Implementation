def compute_metrics(y_true, y_pred):
    """
    Computes precision, recall, and F1 score for binary classification.

    Parameters:
    - y_true: list of actual labels (0 or 1)
    - y_pred: list of predicted labels (0 or 1)

    Returns:
    - precision: float
    - recall: float
    - f1_score: float
    """
    assert len(y_true) == len(y_pred), "Mismatched lengths"

    TP = sum((yt == 1 and yp == 1) for yt, yp in zip(y_true, y_pred))
    FP = sum((yt == 0 and yp == 1) for yt, yp in zip(y_true, y_pred))
    FN = sum((yt == 1 and yp == 0) for yt, yp in zip(y_true, y_pred))

    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1_score


# Example usage
y_true = [1, 0, 1, 1, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1]

precision, recall, f1 = compute_metrics(y_true, y_pred)

print(f"Precision: {precision:.2f}")
print(f"Recall:    {recall:.2f}")
print(f"F1 Score:  {f1:.2f}")
